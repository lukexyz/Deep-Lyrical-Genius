{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`REMEMBER` Ctrl+Shift+P in notebooks to bring up finder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_trf_distilbertbaseuncased_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeForce GTX 1050 Ti\n",
      "pytorch 1.4.0, spacy 2.2.3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import torch\n",
    "\n",
    "torch.cuda.device(0)\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(f'pytorch {torch.__version__}, spacy {spacy.__version__}')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.77565545\n",
      "(16, 768)\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_trf_distilbertbaseuncased_lg')\n",
    "doc = nlp(\"Apple shares rose on the news. Apple pie is delicious.\")\n",
    "print(doc[0].similarity(doc[7]))\n",
    "print(doc._.trf_last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer learning\n",
    "The main use case for pretrained transformer models is transfer learning. You load in a large generic model pretrained on lots of text, and start training on your smaller dataset with labels specific to your problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import GPUtil\n",
    "import torch\n",
    "import numpy\n",
    "from numpy.testing import assert_almost_equal\n",
    "from scipy.spatial import distance\n",
    "import cupy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU!\n",
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  3% | 63% |\n"
     ]
    }
   ],
   "source": [
    "is_using_gpu = spacy.prefer_gpu()\n",
    "if is_using_gpu:\n",
    "    print(\"Using GPU!\")\n",
    "    torch.set_default_tensor_type(\"torch.cuda.FloatTensor\")\n",
    "    print(\"GPU Usage\")\n",
    "    GPUtil.showUtilization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT uses wordpieces (e.g. playing -> play + ##ing) instead of words. This is effective in reducing the size of the vocabulary and increases the amount of data that is available for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'here', 'is', 'some', 'text', 'to', 'en', '##code', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nlp = spacy.load(\"en_trf_distilbertbaseuncased_lg\")\n",
    "doc = nlp(\"Here is some text to encode.\")\n",
    "\n",
    "assert doc.tensor.shape == (7, 768)  # Always has one row per token\n",
    "print(doc._.trf_word_pieces_)        # String values of the wordpieces\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw transformer output has one row per wordpiece."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2182, 2003, 2070, 3793, 2000, 4372, 16044, 1012, 102]\n",
      "[[1], [2], [3], [4], [5], [6, 7], [8]]\n"
     ]
    }
   ],
   "source": [
    "print(doc._.trf_word_pieces)  # Wordpiece IDs (note: *not* spaCy's hash values!)\n",
    "print(doc._.trf_alignment)    # Alignment between spaCy tokens and wordpieces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Sentiment Classifier using spaCy Transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import thinc\n",
    "import random\n",
    "import spacy\n",
    "import GPUtil\n",
    "import torch\n",
    "from spacy.util import minibatch\n",
    "from tqdm.auto import tqdm\n",
    "import unicodedata\n",
    "import wasabi\n",
    "import numpy\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 11% | 17% |\n"
     ]
    }
   ],
   "source": [
    "spacy.util.fix_random_seed(0)\n",
    "is_using_gpu = spacy.prefer_gpu()\n",
    "if is_using_gpu:\n",
    "    torch.set_default_tensor_type(\"torch.cuda.FloatTensor\")\n",
    "    print(\"GPU Usage\")\n",
    "    GPUtil.showUtilization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load IMDB movie database for sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _prepare_partition(text_label_tuples, *, preprocess=False):\n",
    "    texts, labels = zip(*text_label_tuples)\n",
    "    cats = [{\"POSITIVE\": bool(y), \"NEGATIVE\": not bool(y)} for y in labels]\n",
    "    return texts, cats\n",
    "\n",
    "def load_data(*, limit=0, dev_size=2000):\n",
    "    \"\"\"Load data from the IMDB dataset, splitting off a held-out set.\"\"\"\n",
    "    if limit != 0:\n",
    "        limit += dev_size\n",
    "    assert dev_size != 0\n",
    "    train_data, _ = thinc.extra.datasets.imdb(limit=limit)\n",
    "    assert len(train_data) > dev_size\n",
    "    random.shuffle(train_data)\n",
    "    dev_data = train_data[:dev_size]\n",
    "    train_data = train_data[dev_size:]\n",
    "    train_texts, train_labels = _prepare_partition(train_data, preprocess=False)\n",
    "    dev_texts, dev_labels = _prepare_partition(dev_data, preprocess=False)\n",
    "    return (train_texts, train_labels), (dev_texts, dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(train_texts, train_cats), (eval_texts, eval_cats) = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `Models`  \n",
    "en_trf_distilbertbaseuncased_lg  \n",
    "en_trf_xlnetbasecased_lg  \n",
    "en_trf_bertbaseuncased_lg  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# !python -m spacy download en_trf_xlnetbasecased_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO Try with distilbert\n",
    "model_choice = \"en_trf_distilbertbaseuncased_lg\" \n",
    "\n",
    "model_choice = \"en_trf_xlnetbasecased_lg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nlp = spacy.load(model_choice)\n",
    "\n",
    "textcat = nlp.create_pipe(\"trf_textcat\", config={\"architecture\": \"softmax_class_vector\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_trf_xlnetbasecased_lg'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-195d6ca695cf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_choice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpipe_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Loaded model '{model_choice}'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mmodel_choice\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"en_trf_xlnetbasecased_lg\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\deeplyrics\\lib\\site-packages\\spacy\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(name, **overrides)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mdeprecation_warning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\deeplyrics\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(name, **overrides)\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"exists\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Path or Path-like to model data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'en_trf_xlnetbasecased_lg'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(model_choice)\n",
    "\n",
    "print(nlp.pipe_names)\n",
    "print(f\"Loaded model '{model_choice}'\")\n",
    "if model_choice == \"en_trf_xlnetbasecased_lg\":\n",
    "    textcat = nlp.create_pipe(\n",
    "          \"trf_textcat\", config={\"architecture\": \"softmax_class_vector\"}\n",
    "      )\n",
    "elif model_choice == \"en_trf_bertbaseuncased_lg\":\n",
    "    textcat = nlp.create_pipe(\n",
    "          \"trf_textcat\", config={\"architecture\": \"softmax_class_vector\"}\n",
    "      )\n",
    "elif model_choice == \"en_trf_distilbertbaseuncased_lg\":\n",
    "    textcat = nlp.create_pipe(\n",
    "          \"trf_textcat\", config={\"architecture\": \"softmax_class_vector\"}\n",
    "      )\n",
    "else: \n",
    "    print(\"Choose a supported transformer model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add label to text classifier\n",
    "textcat.add_label(\"POSITIVE\")\n",
    "textcat.add_label(\"NEGATIVE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Labels:\", textcat.labels)\n",
    "nlp.add_pipe(textcat, last=True)\n",
    "print(f\"Using {len(train_texts)} training docs, {len(eval_texts)} evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_words = sum(len(text.split()) for text in train_texts)\n",
    "train_data = list(zip(train_texts, [{\"cats\": cats} for cats in train_cats]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_iter=4\n",
    "n_texts=75     # Changed number of texts from 1000 to 75 to relieve pressue on GPU memory\n",
    "batch_size=4   # batch-szie changed from 8 to 4 to relieve pressure on GPU memory\n",
    "learn_rate=2e-5\n",
    "max_wpb=1000\n",
    "pos_label=\"POSITIVE\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cyclic_triangular_rate(min_lr, max_lr, period):\n",
    "    it = 1\n",
    "    while True:\n",
    "        # https://towardsdatascience.com/adaptive-and-cyclical-learning-rates-using-pytorch-2bf904d18dee\n",
    "        cycle = numpy.floor(1 + it / (2 * period))\n",
    "        x = numpy.abs(it / period - 2 * cycle + 1)\n",
    "        relative = max(0, 1 - x)\n",
    "        yield min_lr + (max_lr - min_lr) * relative\n",
    "        it += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(nlp, texts, cats, pos_label):\n",
    "    tp = 0.0  # True positives\n",
    "    fp = 0.0  # False positives\n",
    "    fn = 0.0  # False negatives\n",
    "    tn = 0.0  # True negatives\n",
    "    total_words = sum(len(text.split()) for text in texts)\n",
    "    with tqdm(total=total_words, leave=False) as pbar:\n",
    "        for i, doc in enumerate(nlp.pipe(texts, batch_size=batch_size)):\n",
    "            gold = cats[i]\n",
    "            for label, score in doc.cats.items():\n",
    "                if label not in gold:\n",
    "                    continue\n",
    "                if label != pos_label:\n",
    "                    continue\n",
    "                if score >= 0.5 and gold[label] >= 0.5:\n",
    "                    tp += 1.0\n",
    "                elif score >= 0.5 and gold[label] < 0.5:\n",
    "                    fp += 1.0\n",
    "                elif score < 0.5 and gold[label] < 0.5:\n",
    "                    tn += 1\n",
    "                elif score < 0.5 and gold[label] >= 0.5:\n",
    "                    fn += 1\n",
    "            pbar.update(len(doc.text.split()))\n",
    "    precision = tp / (tp + fp + 1e-8)\n",
    "    recall = tp / (tp + fn + 1e-8)\n",
    "    if (precision + recall) == 0:\n",
    "        f_score = 0.0\n",
    "    else:\n",
    "        f_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return {\"textcat_p\": precision, \"textcat_r\": recall, \"textcat_f\": f_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the TextCategorizer, and create an optimizer.\n",
    "optimizer = nlp.resume_training()\n",
    "optimizer.alpha = 0.001\n",
    "optimizer.trf_weight_decay = 0.005\n",
    "optimizer.L2 = 0.0\n",
    "learn_rates = cyclic_triangular_rate(\n",
    "    learn_rate / 3, learn_rate * 3, 2 * len(train_data) // batch_size\n",
    "    )\n",
    "print(\"Training the model...\")\n",
    "print(\"{:^5}\\t{:^5}\\t{:^5}\\t{:^5}\".format(\"LOSS\", \"P\", \"R\", \"F\"))\n",
    "\n",
    "pbar = tqdm(total=100, leave=False)\n",
    "results = []\n",
    "epoch = 0\n",
    "step = 0\n",
    "eval_every = 100\n",
    "patience = 3\n",
    "while True:\n",
    "    # Train and evaluate\n",
    "    losses = Counter()\n",
    "    random.shuffle(train_data)\n",
    "    batches = minibatch(train_data, size=batch_size)\n",
    "    for batch in batches:\n",
    "        optimizer.trf_lr = next(learn_rates)\n",
    "        texts, annotations = zip(*batch)\n",
    "        nlp.update(texts, annotations, sgd=optimizer, drop=0.1, losses=losses)\n",
    "        pbar.update(1)\n",
    "        if step and (step % eval_every) == 0:\n",
    "            pbar.close()\n",
    "            with nlp.use_params(optimizer.averages):\n",
    "                scores = evaluate(nlp, eval_texts, eval_cats, pos_label)\n",
    "            results.append((scores[\"textcat_f\"], step, epoch))\n",
    "            print(\n",
    "                \"{0:.3f}\\t{1:.3f}\\t{2:.3f}\\t{3:.3f}\".format(\n",
    "                    losses[\"trf_textcat\"],\n",
    "                    scores[\"textcat_p\"],\n",
    "                    scores[\"textcat_r\"],\n",
    "                    scores[\"textcat_f\"],\n",
    "                )\n",
    "            )\n",
    "            pbar = tqdm(total=eval_every, leave=False)\n",
    "        step += 1\n",
    "    epoch += 1\n",
    "    print(f\"epoch {epoch}\")\n",
    "    # Stop if no improvement in HP.patience checkpoints\n",
    "    if results:\n",
    "        best_score, best_step, best_epoch = max(results)\n",
    "        print(f\"best score: {best_score}  best_step : {best_step}  best epoch : {best_epoch} \")\n",
    "        print(f\"break clause: {((step - best_step) // eval_every)}\")\n",
    "        if ((step - best_step) // eval_every) >= patience:\n",
    "            break\n",
    "\n",
    "    msg = wasabi.Printer()\n",
    "    table_widths = [2, 4, 6]\n",
    "    msg.info(f\"Best scoring checkpoints\")\n",
    "    msg.row([\"Epoch\", \"Step\", \"Score\"], widths=table_widths)\n",
    "    msg.row([\"-\" * width for width in table_widths])\n",
    "    for score, step, epoch in sorted(results, reverse=True)[:10]:\n",
    "        msg.row([epoch, step, \"%.2f\" % (score * 100)], widths=table_widths)\n",
    "\n",
    "    # Test the trained model\n",
    "    test_text = eval_texts[0]\n",
    "    doc = nlp(test_text)\n",
    "    print(test_text, doc.cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sources & More information**  \n",
    "*XL-Net explanation*  \n",
    "https://mlexplained.com/2019/06/30/paper-dissected-xlnet-generalized-autoregressive-pretraining-for-language-understanding-explained/  \n",
    "Attention is all you need  \n",
    "https://arxiv.org/abs/1706.03762  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
